# Introduction

<?rfc toc="yes"?>
<?rfc symrefs="yes"?>
<?rfc sortrefs="yes"?>
<?rfc subcompact="no"?>
<?rfc compact="yes"?>
<?rfc comments="yes"?>

The FQ-CoDel algorithm is a combined packet scheduler and AQM
developed as part of the bufferbloat-fighting community effort. It is
based on a modified Deficit Round Robin (DRR) stochastic fairness
queueing scheduler, with the CoDel AQM algorithm operating on each
sub-queue. This document describes the combined algorithm; a reference
implementation is included in the mainline Linux kernel as the
FQ-CoDel queueing discipline.

The rest of this document is structured as follows: The rest of this
section gives some concepts and terminology used in the rest of the
document, and gives a short informal summary of the FQ-CoDel
algorithm. Section 2 (FIXME: automatic internal links?) gives an
overview of the CoDel algorithm. Section 3 defines the parameters and
data structures employed by FQ-CoDel and section 4 describes the
working of the algorithm in detail. Section 6 describes security
considerations while section 8 concludes.

## Terminology and concepts

Flow: A flow is typically identified by a 5-tuple of source IP,
destination IP, source port, destination port, and protocol. It can also
be identified by a superset or subset of those parameters, or by mac
address, or other means.

Queue: A queue of packets represented internally in FQ-CoDel. In most
instances each flow gets its own queue; however because of the
possibility of hash collisions, this is not always the case. In an
attempt to avoid confusion, the word 'queue' is used to refer to the
internal data structure, and 'flow' to refer to the actual stream of
packets being delivered to the FQ-CoDel algorithm.

Scheduler: A mechanism to select which queue a packet is dequeued from.

CoDel AQM: The Active Queue Management algorithm employed by FQ-CoDel.

DRR: Deficit round-robin scheduling.

Packet: An indivisable quantity of bytes.

Quantum: The amount of bytes in a flow that will be delivered at once.

## Informal summary of FQ-CoDel

FQ-CoDel is a *hybrid* of [DRR](#DRR) and [CODEL](#CODEL2012), with an
optimisation for sparse flows similar to [SQF](#SQF2012). We call this "Flow
Queueing" rather than "Fair Queueing" as it is *unfair* to flows that
build a queue.

FQ-CoDel stochastically classifies incoming packets into different
queues by hashing the 5-tuple of IP protocol number and source and
destination IP and port numbers, perturbed with a random number selected
at initiation time. Each queue is managed by the CoDel queueing
discipline. Packet ordering within a queue is preserved, since queues
have FIFO ordering.

The FQ-CoDel algorithm consists of two logical parts: the scheduler
which selects which queue to dequeue a packet from, and the CoDel AQM
which works on each of the queues. The subtleties of FQ-CoDel are mostly
in the scheduling part, whereas the interaction between the scheduler
and the CoDel algorithm are fairly straight forward: At initiation, the
queues are initialised so that each queue has a separate set of CoDel
state variables. By default, 1024 queues are created, but the current
implementation supports anywhere from 2 to 64k separate queues. Each
queue maintains the state variables throughout its lifetime, and so acts
the same as the non-fq CoDel variant would.

On dequeue, FQ-CoDel selects a queue from which to dequeue by a two-tier
round-robin scheme, in which each queue is allowed to dequeue up to a
configurable quantum of bytes for each iteration.. Deviations from this
quantum is maintained as a deficit for the queue, which serves to make
the fairness scheme byte-based rather than a packet-based. The two-tier
round-robin mechanism distinguishes between "new" queues (which don't
build up a standing queue) and "old" queues, that have queued enough
data to be around for more than one iteration of the round-robin
scheduler.

This new/old queue distinction has a particular consequence for queues
that don't build up more than a quantum of bytes before being visited by
the scheduler: Such queues are removed from the list, and then re-added
as a new queue each time a packet arrives for it, and so will get
priority over queues that do not empty out each round (except for a
minor modification to protect against starvation, detailed below).
Exactly how much data a flow has to send to keep its queue in this state
is somewhat difficult to reason about, because it depends on both the
egress link speed and the number of concurrent flows. However, in
practice many things that are beneficial to have prioritised for typical
internet use (ACKs, DNS lookups, interactive SSH, HTTP requests, ARP,
ICMP) *tend* to fall in this category, which is why FQ-CoDel performs so
well for many practical applications.

This scheduling scheme has some subtlety to it, which is explained in
detail in the remainder of this document. For more analysis of the
consequences of the scheduling, and some performance notes, see the
appendix.

FIXME: Write appendix.

# CoDel

CoDel is described in the the ACM Queue paper, [#CODEL], and Van
Jacobson's IETF presentation. The basic idea is to control queue length,
maintaining sufficient queueing to keep the outgoing link busy, but
avoiding building up the queue beyond that point. This is done by
preferentially dropping packets that remain in the queue for “too long”.

When each new packet arrives, it is marked with its arrival time. Later,
when it is that packet's turn to be dequeued, CoDel computes its sojourn
time (the current time minus the arrival time). If the sojourn time for
packets being dequeued exceeds the _target_ time for a time period of at
least _interval_, one or more packets will be dropped (or marked, if ECN
is enabled) in order to signal the source endpoint to reduce its send
rate. If the sojourn still remains above the target time, additional
packet drops will occur on a schedule computed from an
inverse-square-root control law until either (1) the queue becomes empty
or (2) a packet is encountered with a sojourn time that is less than the
target time. This target time, is normally set to about five
milliseconds, and the interval is normally set to about 100
milliseconds. This approach has proven to be quite effective in a wide
variety of situations.

CoDel tries to drop packets at or near the head of a queue, rather
than at the tail.

                       Tail           Head
                         |              |
                         |              |
                    +----|--------------|----+
      +---------+   |  +-v+--+--+--+--+-v+   |
      |         |   |  |  |  |  |  |  |  |   |
      | Source  +----->|  |  |  |  |  |  |+-------------+
      |         |   |  |  |  |  |  |  |  |   |          |
      +---------+   |  +--+--+--+--+--+--+   |          |
           ^        |                        |          v
           |        |        +--+--+         |   +--------------+
           |        |        |  |  |         |   |              |
           +-----------------+  |  |<------------+  Destination |
                    |        |  |  |         |   |              |
                    |        +--+--+         |   +--------------+
                    +------------------------+


# FQ-CoDel Parameters and data structures

## Parameters

### Interval

The interval has the same semantics as CoDel and is used to ensure that
the measured minimum delay does not become too stale. The minimum delay
must be experienced in the last epoch of length interval. It should be
set on the order of the worst-case RTT through the bottleneck to give
end-points sufficient time to react.

Default interval value is 100 ms.

### Target

The target has the same semantics as CoDel. It is the acceptable minimum
standing/persistent queue delay for each FQ-CoDel Queue. This minimum
delay is identified by tracking the local minimum queue delay that
packets experience.

Default target value is 5 ms.

### Packet limit

Routers do not have infinite memory, so some packet limit must be
enforced.

The packet limit has the same semantics as CoDel and is the hard limit
on the real queue size.  When this limit is reached, incoming packets
from the largest queue (measured in bytes) are dropped from the head
of the flow. Note that this limit is a global limit for packets in all
queues; each individual queue does not have an upper limit.

The default packet limit is 10240 packets.

### Quantum

The quantum is the number of bytes used as 'deficit' in the flow
queuing algorithm. Default is set to 1514 bytes which corresponds to
the Ethernet MTU plus the hardware header length of 14 bytes.

In TSO-enabled systems, where a "packet" consists of an offloaded
packet train, it can presently be as large as 64k bytes. In
GRO-enabled systems, up to 4K bytes.

### Flows

The _flows_ parameter sets the number of sub-queues into which the
incoming packets are classified. Due to the stochastic nature of
hashing, multiple flows may end up being hashed into the same slot.

This parameter can be set only at load time since memory has to be
allocated for the hash table in the current implementation.

The default value is 1024.

FIXME: I've been trying very hard to avoid referring to the internal
sub-queues as 'flows', to distinguish between the flows being hashed
into buckets, and the actual queues. The naming of this parameter makes
this rather difficult...

### ECN

ECN is *enabled* by default. Rather than do anything special with
misbehaved ECN flows, FQ-CoDel relies on the packet scheduling system
to minimize their impact, thus unresponsive packets in a flow being
marked with ECN can grow to the overall packet limit, but will not
otherwise affect the performance of the system.

## Data structures

### Internal sub-queues

The main data structure of FQ-CoDel is the list of sub-queues, which is
instantiated at initiation to the number of queues specified by the
_flows_ parameter. Each sub-queue consists simply an ordered list of
packets with FIFO semantics, two state variables tracking the queue
deficit and total number of bytes enqueued, and the set of CoDel state
variables (which includes an arrival timestamp of each packet in the
queue). Other state variables to track queue statistics can also be
included: the Linux implementation keeps a per-queue count of dropped
packets.

Queue space is shared: there's a global limit on the number of packets
the queues can hold, but not one per queue.

### New and old queues lists

FQ-CoDel maintains two lists of active queues, called "new" and "old"
queues. Each list is an ordered list containing references to the
sub-queues. When a packet is added to a queue that is not currently
active, that queue becomes active by being added to the list of new
queues. Later on, it is moved to the list of old queues, from which it
is removed when it is no longer active. This behaviour is the source of
some subtlety in the packet scheduling at dequeue time, explained below.


# The FQ-CoDel scheduler

This section describes the operation of the FQ-CoDel scheduler. It is
split into three parts: An explanation of the deficit round-robin (DRR)
mechanism employed to divide bandwidth between queues, and two parts
explaining the enqueue and dequeue operations.

## The DRR mechanism

FQ-CoDel is byte-based, employing a deficit round-robin mechanism[3]
between queues. This works by keeping track of the current byte
_deficit_ of each queue. This deficit is initialised to the configurable
quantum; each time a queue gets a dequeue opportunity, it gets to
dequeue packets, decreasing the deficit by the packet size for each
packet, until the deficit runs into the negative, at which point it is
increased by one quantum, and the dequeue opportunity ends.

This means that if one queue contains packets of size quantum/3, and
another contains quantum-sized packets, the first queue will dequeue
three packets each time it gets a turn, whereas the second only dequeues
one. This means that flows that send small packets are not penalised by
the difference in packet sizes; rather, the DRR scheme approximates a
(single-)byte-based fairness queueing. The size of the quantum
determines the scheduling granularity, with the tradeoff from too small
a quantum being scheduling overhead. For small bandwidths, lowering the
quantum from the default MTU size can be advantageous.

## Enqueue

The packet enqueue mechanism consists of three stages: classification
into a sub-queue, timestamping and bookkeeping, and optionally dropping
a packet when the total number of enqueues packets goes over the
maximum.

When a packet is enqueued, it is first classified into the appropriate
sub-queue. By default, this is done by hashing on the 5-tuple of IP
protocol, and source and destination IP and port numbers, permuted by a
random value selected at initialisation time, and taking the hash value
modulo the number of sub-queues. However, an implementation may also
specify a configurable sub-queue classification scheme (the Linux
implementation does so in the form of the 'tc filter' command). In this
case, classification failure results in the packet being dropped and no
further action taken.

FIXME: Should the hashing algorithm me specified?

Once the packet has been successfully classified into a sub-queue, it is
handed over to the CoDel algorithm for timestamping. It is then added to
the tail of the selected queue, and the queue's byte count is updated by
the packet size. Then, if the queue is not currently active (i.e. if it
is not in either the list of new or the list of old queues), it is added
to the end of the list of new queues, and its deficit is initiated to
the configured quantum.

Finally, the total number of enqueued packets is compared with the
configured limit, and if it is *above* this value (which can happen
since a packet was just enqueued), a packet is dropped from the head
of the queue with the largest current byte count. Note that this in most
cases means that the packet that gets dropped is different from the one
that was just enqueued, and may even be from a different queue.

## Dequeue

Most of FQ-CoDel's scheduling is done at packet dequeue time. It
consists of three parts: selecting a queue from which to dequeue a
packet, actually dequeuing it (employing the CoDel algorithm in the
process), and some final bookkeeping.

For the first part, the scheduler first looks at the list of new
queues; for each queue in that list, if that queue has a negative
deficit (i.e. it has already dequeued a quantum of bytes (or more)),
its deficit is increased by one quantum, and the queue is put onto the
end of the list of old queues, and the routine selects the next queue
and starts again.

Otherwise, that queue is selected for dequeue. If the list of new
queues is empty, the scheduler proceeds down the list of old queues in
the same fashion (checking the deficit, and either selecting the queue
for dequeuing, or increasing the deficit and putting the queue back at
the end of the list).

After having selected a queue from which to dequeue a packet, the CoDel
algorithm is invoked on that queue. This applies the CoDel control law,
and may discard one or more packets from the head of the queue, before
returning the packet that should be dequeued (or nothing if the queue is
or becomes empty while being handled by the CoDel algorithm).

Finally, if the CoDel algorithm did not return a packet, the queue is
empty, and the scheduler does one of two things: if the queue selected
for dequeue came from the list of new queues, it is moved to the end of
the list of old queues. If instead it came from the list of old queues,
that queue is removed from the list, to be added back (as a new queue)
the next time a packet arrives that hashes to that queue. Then, (since
no packet was available for dequeue), the whole dequeue process is
restarted from the beginning.

If, instead, the scheduler *did* get a packet back from the CoDel
algorithm, it updates the byte deficit for the selected queue before
returning the packet as the result of the dequeue operation.

Note that the step that moves an empty queue from the list of new queues
to the list of old queues before it is removed, is crucial to prevent
starvation. This is because otherwise the queue can reappear (the next
time a packet arrives for it) before the list of old queues is visited;
this can go on indefinitely even with a small number of active flows, if
the flow providing packets to the queue in question transmits at just
the right rate. This is prevented by first moving the queue to the list
of old queues, thus forcing a pass through that, and preventing
starvation.

The resulting migration of queues between the different states is
summarised in the following state diagram:

    +-----------------+                +--------------------+
    |                 |     Empty      |                    |
    |     Empty       |<---------------+        Old         +-----+
    |                 |                |                    |     |
    +-------+---------+                +--------------------+     |
            |                             ^              ^        |Quantum
            |Arrival                      |              |        |Exceeded
            v                             |              |        |
    +-----------------+                   |              |        |
    |                 |     Empty or      |              |        |
    |      New        +-------------------+              +--------+
    |                 |  Quantum exceeded
    +-----------------+


# Implementation considerations

### Memory Overhead

FQ-CoDel can be implemented with a very low memory footprint (less than
64 bytes per queue on 64 bit systems). This is the data structure used in
the Linux implementation:

	struct fq_codel_flow {
	    struct sk_buff    *head;
		struct sk_buff    *tail;
        struct list_head  flowchain;
        int               deficit;
        u32               dropped; /* number of drops (or ECN marks) on this flow */
        struct codel_vars cvars;
	};

# Resources and Additional Information

# Security Considerations

This document describes an hybrid packet scheduling and active queue
management algorithm for implementation in networked devices. There are
no specific security exposures associated with FQ-CoDel. Some exposures
present in current FIFO systems are in fact reduced (e.g. simple minded
packet floods).

# IANA Considerations

This document has no actions for IANA.

# Acknowlegements

# Conclusions

FQ-CoDel is a very general, efficient, nearly parameterless active
queue management approach combining flow queuing with CoDel. It is a
critical tool in solving bufferbloat.

FQ-CoDel's default settings SHOULD be modified for other special-purpose
networking applications, for use in data centers or in long

On-going projects are: improving FQ-CoDel with more SFQ-like behavior
for lower bandwidth systems, improving the control law, optimizing
sparse packet drop behavior, etc.

[NFQCODEL](http://www.bufferbloat.net/projects/cerowrt/wiki/nfq_codel).
ns2 and ns3 models are available.

### Appendix A - Examples of use

FIXME: Is this really useful? Also, very Linux-specific.

	|   tc qdisc del dev eth0 root
	|  tc qdisc add dev eth0 root fq_codel
	|  tc -s qdisc show
	|       qdisc  fq_codel  8002:  dev  eth0 root refcnt 2 limit 10240p flows 1024
	|       quantum 1514
	|        target 5.0ms interval 100.0ms ecn
	|          Sent 428514 bytes 2269 pkt (dropped 0, overlimits 0 requeues 0)
	|          backlog 0b 0p requeues 0
	|           maxpacket 256 drop_overlimit 0 new_flow_count 0 ecn_mark 0
	|           new_flows_len 0 old_flows_len 0


	|    tc qdisc del dev eth0 root
	|   tc qdisc add dev dev eth0 root fq_codel limit 800  target  20ms  interval 120ms noecn
	|   tc -s qdisc show dev eth0
	|       qdisc  fq_codel  8003:  dev  eth0  root refcnt 2 limit 800p flows 1024
	|      quantum 1514 target 20.0ms interval 120.0ms
	|       Sent 2588985006 bytes 1783629 pkt (dropped 0,  overlimits  0  requeues
	|      34869)
	|        backlog 0b 0p requeues 34869
	|        maxpacket 65226 drop_overlimit 0 new_flow_count 73 ecn_mark 0
	|        new_flows_len 1 old_flows_len 3
